{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geek-ubaid/clustering-analysis/blob/master/Experiment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYnE0hFZ7yr",
        "colab_type": "text"
      },
      "source": [
        "## **Lab Experiment 5**\n",
        "\n",
        "**Name : Ubaid Usmani**<br>\n",
        "**Reg No: 17BBCE0983**\n",
        "\n",
        "#### **Question**\n",
        "\n",
        "Download a dataset from UCI repository of any application implement the following algorithm\n",
        "- K-mode clustering\n",
        "- Gaussian Mixture Model Using the Expectation Maximization\n",
        "- Compare the performances of above and Give your Inferences\n",
        "---\n",
        "\n",
        "**Table of content:**\n",
        "\n",
        "1. <a href=\"#kmeans\">K-mode clustering with scratch</a>\n",
        "2. <a href=\"https://colab.research.google.com/drive/1Kx3fpg-GajByXAVI7TCB8qIb6NV4blug#scrollTo=tKeG-1XGjlTt&line=1&uniqifier=1\">Gaussian Mixture model using the EM algorithm</a> \n",
        "3. <a href=\"\">Comparing results of both the model</a>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKqwz_vUm6Tj",
        "colab_type": "text"
      },
      "source": [
        "## **About Dataset**\n",
        "\n",
        "\n",
        "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
        "\n",
        "<br/>\n",
        "\n",
        "#### **Data set used**\n",
        "\n",
        "bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n",
        "\n",
        "<br/>\n",
        "\n",
        "#### **Attributes**\n",
        "\n",
        "**Input Variables (features)**\n",
        "\n",
        "1. age (numeric)\n",
        "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
        "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
        "4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
        "5. default: has credit in default? (categorical: 'no','yes','unknown')\n",
        "6. housing: has housing loan? (categorical: 'no','yes','unknown')\n",
        "7. loan: has personal loan? (categorical: 'no','yes','unknown')\n",
        "8. contact: contact communication type (categorical: 'cellular','telephone')\n",
        "9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
        "10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
        "11. duration: last contact duration, in seconds (numeric). \n",
        "12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
        "14. previous: number of contacts performed before this campaign and for this client (numeric)\n",
        "15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
        "16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
        "17. cons.price.idx: consumer price index - monthly indicator (numeric)\n",
        "18. cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
        "19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
        "20. nr.employed: number of employees - quarterly indicator (numeric)\n",
        "\n",
        "**Output variable (desired target)**\n",
        "\n",
        "- y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
        "\n",
        "#### **Reference Link**\n",
        "https://archive.ics.uci.edu/ml/datasets/bank+marketing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK90QzcEVl3y",
        "colab_type": "text"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZeZ_RqVVzPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6l4Dk6dVlWx",
        "colab_type": "code",
        "outputId": "3023ee1e-bc91-4843-a6a1-c997663eec9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "dataset_path = '/content/drive/My Drive/ML Experiments/bankmarketing.csv'\n",
        "data = pandas.read_csv(dataset_path)\n",
        "\n",
        "label =data.iloc[:,-1]\n",
        "\n",
        "data_numeric = data.loc[:,data.dtypes!=np.object]\n",
        "print(data_numeric.head())\n",
        "\n",
        "data_cat = data.loc[:,data.dtypes==np.object]\n",
        "print(data_cat.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   age  duration  campaign  ...  cons.conf.idx  euribor3m  nr.employed\n",
            "0   56       261         1  ...          -36.4      4.857       5191.0\n",
            "1   57       149         1  ...          -36.4      4.857       5191.0\n",
            "2   37       226         1  ...          -36.4      4.857       5191.0\n",
            "3   40       151         1  ...          -36.4      4.857       5191.0\n",
            "4   56       307         1  ...          -36.4      4.857       5191.0\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "         job  marital    education  default  ... month day_of_week     poutcome   y\n",
            "0  housemaid  married     basic.4y       no  ...   may         mon  nonexistent  no\n",
            "1   services  married  high.school  unknown  ...   may         mon  nonexistent  no\n",
            "2   services  married  high.school       no  ...   may         mon  nonexistent  no\n",
            "3     admin.  married     basic.6y       no  ...   may         mon  nonexistent  no\n",
            "4   services  married  high.school       no  ...   may         mon  nonexistent  no\n",
            "\n",
            "[5 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twHl-ztxYcXX",
        "colab_type": "text"
      },
      "source": [
        "### **Correlation Analysis for feature selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bh2GeJMYhl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "def chi_square_test(feature1, feature2):\n",
        "  result = []\n",
        "  result.append(feature1)\n",
        "\n",
        "  contingency_table = pd.crosstab(data_cat[feature1], data_cat[feature2])\n",
        "  observed_values = contingency_table.values\n",
        "  chi_square_result = stats.chi2_contingency(contingency_table)\n",
        "  expected_values = chi_square_result[3]\n",
        "\n",
        "  ## calculate degree of freedom ( more the degree more chances of getting good results)\n",
        "\n",
        "  no_rows = len(contingency_table.iloc[0:2,0])\n",
        "  no_columns = len(contingency_table.iloc[0,0:2])\n",
        "\n",
        "  degree_freedom = (no_rows-1) / (no_columns-1)\n",
        "  result.append(degree_freedom)\n",
        "\n",
        "  ## Significance level (Generally used for the test)\n",
        "  alpha=0.05\n",
        "  \n",
        "  chi_square = sum([(o-e)**2./e for o,e in zip(observed_values, expected_values)])\n",
        "  chi_square_statistic = chi_square[0] + chi_square[1]\n",
        "  result.append(chi_square_statistic)\n",
        "\n",
        "  critical_value = stats.chi2.ppf(q = 1-alpha, df=degree_freedom)\n",
        "  result.append(critical_value)\n",
        "\n",
        "  p_value = 1 - stats.chi2.cdf(x = chi_square_statistic, df = degree_freedom)\n",
        "  result.append(p_value) \n",
        "\n",
        "  ## Check for Null hypothesis using p_value and chi_square statistics\n",
        "\n",
        "  if chi_square_statistic >= critical_value or p_value <= alpha:\n",
        "    result.append('True')\n",
        "    return result\n",
        "  else:\n",
        "    result.append('False')\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLHL1OXpYbq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_cat = []\n",
        "for i in data_cat.columns.tolist()[:-1]:\n",
        "  corr_cat.append(chi_square_test(i,'y'))\n",
        "\n",
        "cat_corr = pd.DataFrame(corr_cat, columns=['Feature' , 'DOF', 'Chi_Sq-Stats', 'critical_val', 'p_value', 'Correlated'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn1Kv1i7qITw",
        "colab_type": "text"
      },
      "source": [
        "#### **Correlation of all categorical variable with the target variable 'y'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leHd4nTcp_6C",
        "colab_type": "code",
        "outputId": "12d860a3-998e-449a-9e66-0fe380ddecbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "cat_corr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>DOF</th>\n",
              "      <th>Chi_Sq-Stats</th>\n",
              "      <th>critical_val</th>\n",
              "      <th>p_value</th>\n",
              "      <th>Correlated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>job</td>\n",
              "      <td>1.0</td>\n",
              "      <td>961.242440</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>marital</td>\n",
              "      <td>1.0</td>\n",
              "      <td>122.655152</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>education</td>\n",
              "      <td>1.0</td>\n",
              "      <td>193.105905</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>default</td>\n",
              "      <td>1.0</td>\n",
              "      <td>406.577515</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>housing</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.684496</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>1.711546e-02</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>loan</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.094028</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>2.955806e-01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>contact</td>\n",
              "      <td>1.0</td>\n",
              "      <td>863.269081</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>month</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3101.149351</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>day_of_week</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.144939</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>3.167261e-07</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>poutcome</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4230.523798</td>\n",
              "      <td>3.841459</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Feature  DOF  Chi_Sq-Stats  critical_val       p_value Correlated\n",
              "0          job  1.0    961.242440      3.841459  0.000000e+00       True\n",
              "1      marital  1.0    122.655152      3.841459  0.000000e+00       True\n",
              "2    education  1.0    193.105905      3.841459  0.000000e+00       True\n",
              "3      default  1.0    406.577515      3.841459  0.000000e+00       True\n",
              "4      housing  1.0      5.684496      3.841459  1.711546e-02       True\n",
              "5         loan  1.0      1.094028      3.841459  2.955806e-01      False\n",
              "6      contact  1.0    863.269081      3.841459  0.000000e+00       True\n",
              "7        month  1.0   3101.149351      3.841459  0.000000e+00       True\n",
              "8  day_of_week  1.0     26.144939      3.841459  3.167261e-07       True\n",
              "9     poutcome  1.0   4230.523798      3.841459  0.000000e+00       True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZxqdvo9reRM",
        "colab_type": "text"
      },
      "source": [
        "So leaving loan all other variable passed the correlation test and can be used in clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7iKyEjWYavL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replace_map = {'y': {'yes': 1, 'no': 0}}\n",
        "data.replace(replace_map, inplace=True)\n",
        "data['age_bin'] = pandas.cut(data['age'], [0, 20, 30, 40, 50, 60, 70, 80, 90, 100], \n",
        "                            labels=['0-20', '20-30', '30-40', '40-50','50-60','60-70','70-80', '80-90','90-100'])\n",
        "\n",
        "data  = data.drop('age',axis = 1)\n",
        "\n",
        "features = data[['age_bin','job', 'marital', 'education',\\\n",
        "              'default', 'housing','contact',\\\n",
        "              'month','day_of_week','poutcome','y']]\n",
        "\n",
        "le = LabelEncoder()\n",
        "features_encoded = features.apply(le.fit_transform)\n",
        "\n",
        "\n",
        "label=data[['y']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWocvyDLi45T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<a id=\"kmeans\"></a>\n",
        "\n",
        "## **K-modes Clustering (Without library implementation)**\n",
        "\n",
        "K-Modes algorithm is a clustering used to cluster data based on dissimilarity among the cluster data. The smaller the number more chances are there of them being in the same cluster. Here the mode function is used to cluster data. A mode is a vector of elements that minimizes the dissimilarities between the vector itself and each object of the data. We will have as many modes as the number of clusters we required, since they act as centroids.\n",
        "<p align='center'>\n",
        "<img src='https://i.ytimg.com/vi/KK4wfiQqUN0/maxresdefault.jpg'/>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_3hPJElnBxL",
        "colab_type": "text"
      },
      "source": [
        "### **Packages Importing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETKG2qfO2T5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import random \n",
        "from collections import defaultdict, Counter, deque\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas\n",
        "from scipy import stats\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPVwoNVunMBZ",
        "colab_type": "text"
      },
      "source": [
        "### **Utils Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylx9gtqr2_pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def pickup_centroids(df,k):\n",
        "    centroids_idx = np.random.choice(a = df.index.values, replace = False, size = k)\n",
        "    centroid_a = df.loc[centroids_idx[0]].values\n",
        "    centroid_b = df.loc[centroids_idx[1]].values\n",
        "    return(centroid_a,centroid_b)\n",
        "\n",
        "def distance_mismatch(a,b):\n",
        "    return (a != b).sum()\n",
        "\n",
        "def compute_distances_to_centroids(centroid_a,centroid_b,df):\n",
        "    \n",
        "    dic_distances = {}\n",
        "    for idx,row in df.iterrows():\n",
        "        candidat = row.values\n",
        "        distance_to_a = distance_mismatch(candidat[:-1],centroid_a[:-1])\n",
        "        distance_to_b = distance_mismatch(candidat[:-1],centroid_b[:-1])\n",
        "        affectation = np.argmin([distance_to_a,distance_to_b])\n",
        "        dic_distances[idx] = {\"distance_to_a\" : distance_to_a,\n",
        "                              \"distance_to_b\" : distance_to_b,\n",
        "                              \"cluster\" : affectation}\n",
        "        \n",
        "    return dic_distances\n",
        "\n",
        "\n",
        "def extract_assigned_data(dic_distances,df):\n",
        "    c1list = []\n",
        "    c2list = []\n",
        "    for k,v in dic_distances.items():\n",
        "        if v[\"cluster\"] == 0:\n",
        "            c1list.append(df.loc[k].values)\n",
        "        else:\n",
        "            c2list.append(df.loc[k].values)\n",
        "\n",
        "    if len(c1list)>0:\n",
        "        a = np.vstack(c1list)\n",
        "    else:\n",
        "        a = np.array([])\n",
        "        \n",
        "    if len(c2list)>0:\n",
        "        b = np.vstack(c2list)\n",
        "    else:\n",
        "        b = np.array([])\n",
        "        \n",
        "    return a,b\n",
        "\n",
        "\n",
        "def compute_mode(array):\n",
        "    return stats.mode(array)[0][0]\n",
        "\n",
        "\n",
        "def compute_performance(dic_distances):\n",
        "    \n",
        "    distances_list = []\n",
        "    for k,v in dic_distances.items():\n",
        "        if v[\"cluster\"] == 0:\n",
        "            distances_list.append(v[\"distance_to_a\"])\n",
        "        else:\n",
        "            distances_list.append(v[\"distance_to_b\"])\n",
        "    return (np.array(distances_list)).sum()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQw5iwyorUXx",
        "colab_type": "text"
      },
      "source": [
        "## **Main Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay9GXFodPDQ5",
        "colab_type": "code",
        "outputId": "1e27159a-de66-4bae-8f89-6c4e85aeb01d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%timeit\n",
        "def kmodes(df,k=2,threshold=1,iterations=5, verbose = True):\n",
        "   \n",
        "  dic_results = {}  \n",
        "  centroid_a,centroid_b = pickup_centroids(df,k)\n",
        "\n",
        "  for i in range(iterations):\n",
        "\n",
        "    if verbose:\n",
        "      print(\"iteration : \",i)\n",
        "\n",
        "      dic_distances = compute_distances_to_centroids(centroid_a,centroid_b,df)\n",
        "      array_a,array_b = extract_assigned_data(dic_distances,df)\n",
        "      mycdt = len(array_a)==0 or len(array_b)==0\n",
        "\n",
        "      if mycdt:\n",
        "        continue\n",
        "\n",
        "      futur_centroid_a = compute_mode(array_a)\n",
        "      futur_centroid_b = compute_mode(array_b)\n",
        "\n",
        "      print('iteration : ' + str(i) + ' - complete')\n",
        "\n",
        "      d = distance_mismatch(futur_centroid_a,centroid_a) + distance_mismatch(futur_centroid_b,centroid_b)\n",
        "\n",
        "      if verbose:\n",
        "        print(\"distance between present and new centroids : \", d)\n",
        "\n",
        "      if d<threshold:\n",
        "        break\n",
        "      \n",
        "      centroid_a = futur_centroid_a\n",
        "      centroid_b = futur_centroid_b\n",
        "                 \n",
        "  print(\"Total elements in cluster 1 \" + str(len(array_a)))\n",
        "  print(\"Total elements in cluster 2 \" + str(len(array_b)))\n",
        "\n",
        "  df_pred = df.copy()\n",
        "  df_pred['pred_cluster'] = pandas.Series(np.random.randn(len(df)), index=df_pred.index)\n",
        "  for idx,row in df_pred.iterrows():\n",
        "    df_pred.iloc[idx,-1] = dic_distances[idx]['cluster']\n",
        "\n",
        "  return df_pred\n",
        "\n",
        "  \n",
        "def main():\n",
        "\n",
        "  df = kmodes(features, verbose = True)\n",
        "  print(df.head())\n",
        "  return df\n",
        "\n",
        "kmodes_clsuter_df = main()\n",
        "kmodes_clsuter_df['y'] = kmodes_clsuter_df['y'].astype('float16')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration :  0\n",
            "iteration : 0 - complete\n",
            "distance between present and new centroids :  5\n",
            "iteration :  1\n",
            "iteration : 1 - complete\n",
            "distance between present and new centroids :  0\n",
            "Total elements in cluster 1 22562\n",
            "Total elements in cluster 2 18626\n",
            "  age_bin        job  marital  ...     poutcome  y pred_cluster\n",
            "0   50-60  housemaid  married  ...  nonexistent  0          0.0\n",
            "1   50-60   services  married  ...  nonexistent  0          0.0\n",
            "2   30-40   services  married  ...  nonexistent  0          0.0\n",
            "3   30-40     admin.  married  ...  nonexistent  0          0.0\n",
            "4   50-60   services  married  ...  nonexistent  0          0.0\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "iteration :  0\n",
            "iteration : 0 - complete\n",
            "distance between present and new centroids :  6\n",
            "iteration :  1\n",
            "iteration : 1 - complete\n",
            "distance between present and new centroids :  0\n",
            "Total elements in cluster 1 26726\n",
            "Total elements in cluster 2 14462\n",
            "  age_bin        job  marital  ...     poutcome  y pred_cluster\n",
            "0   50-60  housemaid  married  ...  nonexistent  0          1.0\n",
            "1   50-60   services  married  ...  nonexistent  0          1.0\n",
            "2   30-40   services  married  ...  nonexistent  0          1.0\n",
            "3   30-40     admin.  married  ...  nonexistent  0          1.0\n",
            "4   50-60   services  married  ...  nonexistent  0          1.0\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "iteration :  0\n",
            "iteration : 0 - complete\n",
            "distance between present and new centroids :  2\n",
            "iteration :  1\n",
            "iteration : 1 - complete\n",
            "distance between present and new centroids :  0\n",
            "Total elements in cluster 1 30367\n",
            "Total elements in cluster 2 10821\n",
            "  age_bin        job  marital  ...     poutcome  y pred_cluster\n",
            "0   50-60  housemaid  married  ...  nonexistent  0          0.0\n",
            "1   50-60   services  married  ...  nonexistent  0          0.0\n",
            "2   30-40   services  married  ...  nonexistent  0          0.0\n",
            "3   30-40     admin.  married  ...  nonexistent  0          0.0\n",
            "4   50-60   services  married  ...  nonexistent  0          0.0\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "iteration :  0\n",
            "iteration : 0 - complete\n",
            "distance between present and new centroids :  4\n",
            "iteration :  1\n",
            "iteration : 1 - complete\n",
            "distance between present and new centroids :  0\n",
            "Total elements in cluster 1 28936\n",
            "Total elements in cluster 2 12252\n",
            "  age_bin        job  marital  ...     poutcome  y pred_cluster\n",
            "0   50-60  housemaid  married  ...  nonexistent  0          0.0\n",
            "1   50-60   services  married  ...  nonexistent  0          0.0\n",
            "2   30-40   services  married  ...  nonexistent  0          0.0\n",
            "3   30-40     admin.  married  ...  nonexistent  0          0.0\n",
            "4   50-60   services  married  ...  nonexistent  0          0.0\n",
            "\n",
            "[5 rows x 12 columns]\n",
            "1 loop, best of 3: 51.1 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PjNyBXM0dgw",
        "colab_type": "code",
        "outputId": "fc8485e1-f8c6-4dc0-cb33-9d9590ab2b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "train_accuracy = np.mean(kmodes_clsuter_df['pred_cluster'].ravel() == kmodes_clsuter_df['y'].ravel()) * 100\n",
        "\n",
        "print('train accuracy : ' , train_accuracy)\n",
        "\n",
        "print(classification_report(kmodes_clsuter_df['pred_cluster'], kmodes_clsuter_df['y']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy :  59.71885015052928\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.91      0.73     24497\n",
            "         1.0       0.51      0.14      0.22     16691\n",
            "\n",
            "    accuracy                           0.60     41188\n",
            "   macro avg       0.56      0.52      0.48     41188\n",
            "weighted avg       0.57      0.60      0.52     41188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKeG-1XGjlTt",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"gmm\"></a>\n",
        "## **Gaussian mixture model using EM**\n",
        "\n",
        "Gaussian Mixture Models (GMMs) assume that there are a certain number of Gaussian distributions, and each of these distributions represent a cluster. Hence, a Gaussian Mixture Model tends to group the data points belonging to a single distribution together.\n",
        "<br/>\n",
        "\n",
        "<p align='center'>\n",
        "<img src='https://www.researchgate.net/profile/Jeremie_Sublime/publication/310644322/figure/fig4/AS:431286679543811@1479838165688/Illustration-of-the-EM-algorithm-GMM-on-the-Old-Faithful-data-set.png'/ height=50% width=50%>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxpPp5KZ4eEE",
        "colab_type": "text"
      },
      "source": [
        "## **Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgjZ8NSR4dLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats as sp\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing, mixture\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW3F7-Ta4V-H",
        "colab_type": "text"
      },
      "source": [
        "## **Main Algorithm**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJHGLaPu4NiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussianMixModel(object):\n",
        "    def __init__(self, X, k=2):\n",
        "        # Algorithm can work for any number of columns in dataset\n",
        "        X = np.asarray(X)\n",
        "        self.m, self.n = X.shape\n",
        "        self.data = X.copy()\n",
        "        print (np.mean(X))\n",
        "        # number of mixtures\n",
        "        self.k = k\n",
        "\n",
        "    def _init(self):\n",
        "        # init mixture means/sigmas\n",
        "        self.mean_arr = np.asmatrix(np.random.random((self.k, self.n))+np.mean(self.data))\n",
        "        #self.mean_arr[0]=0;\n",
        "        #self.mean_arr[1]=25;\n",
        "        self.sigma_arr = np.array([np.asmatrix(np.identity(self.n)) for i in range(self.k)])\n",
        "        self.phi = np.ones(self.k)/self.k\n",
        "        self.Z = np.asmatrix(np.empty((self.m, self.k), dtype=float))\n",
        "        #Z Latent Variable giving probability of each point for each distribution\n",
        "\n",
        "    def fit(self, tol=1e-4):\n",
        "        # Algorithm will run unti max of log-likelihood is achieved\n",
        "        self._init()\n",
        "        num_iters = 0\n",
        "        logl = 1\n",
        "        previous_logl = 0\n",
        "        while(logl-previous_logl > tol):\n",
        "            previous_logl = self.loglikelihood()\n",
        "            self.e_step()\n",
        "            self.m_step()\n",
        "            num_iters += 1\n",
        "            logl = self.loglikelihood()\n",
        "            print('Iteration %d: log-likelihood is %.6f'%(num_iters, logl))\n",
        "        print('Terminate at %d-th iteration:log-likelihood is %.6f'%(num_iters, logl))\n",
        "\n",
        "    def loglikelihood(self):\n",
        "        logl = 0\n",
        "        for i in range(self.m):\n",
        "            tmp = 0\n",
        "            for j in range(self.k):\n",
        "                #print(self.sigma_arr[j])\n",
        "                tmp += sp.multivariate_normal.pdf(self.data[i, :],self.mean_arr[j, :].A1,self.sigma_arr[j, :]) * self.phi[j]\n",
        "            logl += np.log(tmp)\n",
        "        return logl\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def e_step(self):\n",
        "        #Finding probability of each point belonging to each pdf and putting it in latent variable Z\n",
        "        for i in range(self.m):\n",
        "            den = 0\n",
        "            for j in range(self.k):\n",
        "                #print (self.data[i, :])\n",
        "                num = sp.multivariate_normal.pdf(self.data[i, :],\n",
        "                                                       self.mean_arr[j].A1,\n",
        "                                                       self.sigma_arr[j]) *\\\n",
        "                      self.phi[j]\n",
        "                den += num\n",
        "\n",
        "                self.Z[i, j] = num\n",
        "            self.Z[i, :] /= den\n",
        "            assert self.Z[i, :].sum() - 1 < 1e-4  # Program stop if this condition is false\n",
        "\n",
        "    def m_step(self):\n",
        "        #Updating mean and variance\n",
        "        for j in range(self.k):\n",
        "            const = self.Z[:, j].sum()\n",
        "            self.phi[j] = 1/self.m * const\n",
        "            _mu_j = np.zeros(self.n)\n",
        "            _sigma_j = np.zeros((self.n, self.n))\n",
        "            for i in range(self.m):\n",
        "                _mu_j += (self.data[i, :] * self.Z[i, j])\n",
        "                _sigma_j += self.Z[i, j] * ((self.data[i, :] - self.mean_arr[j, :]).T * (self.data[i, :] - self.mean_arr[j, :]))\n",
        "\n",
        "            self.mean_arr[j] = _mu_j / const\n",
        "            self.sigma_arr[j] = _sigma_j / const\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxNHvY_t81Mu",
        "colab_type": "text"
      },
      "source": [
        "## **Testing and visualizing GMM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YkNaX-M3Vof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cluster_gmm(No_Component=2):\n",
        "  \n",
        "    gmm = mixture.GaussianMixture(n_components=2)\n",
        "    gmm.fit(features_encoded)\n",
        "    \n",
        "    y_train_pred = gmm.predict(features_encoded)\n",
        "\n",
        "    print(classification_report(y_train_pred, label))\n",
        "\n",
        "    return gmm, y_train_pred\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkOVe2KeS27f",
        "colab_type": "code",
        "outputId": "bd37ab56-05be-454d-e631-49e5340e67d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "%%timeit\n",
        "gmm_model, gmm_preds = cluster_gmm(2)\n",
        "\n",
        "gmm_df = features.copy()\n",
        "\n",
        "gmm_df['preds'] = gmm_preds\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.90      0.63     19303\n",
            "           1       0.60      0.13      0.21     21885\n",
            "\n",
            "    accuracy                           0.49     41188\n",
            "   macro avg       0.54      0.52      0.42     41188\n",
            "weighted avg       0.55      0.49      0.41     41188\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.90      0.63     19303\n",
            "           1       0.60      0.13      0.21     21885\n",
            "\n",
            "    accuracy                           0.49     41188\n",
            "   macro avg       0.54      0.52      0.42     41188\n",
            "weighted avg       0.55      0.49      0.41     41188\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.87      0.65     21850\n",
            "           1       0.40      0.10      0.15     19338\n",
            "\n",
            "    accuracy                           0.51     41188\n",
            "   macro avg       0.46      0.48      0.40     41188\n",
            "weighted avg       0.46      0.51      0.42     41188\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.87      0.65     21850\n",
            "           1       0.40      0.10      0.15     19338\n",
            "\n",
            "    accuracy                           0.51     41188\n",
            "   macro avg       0.46      0.48      0.40     41188\n",
            "weighted avg       0.46      0.51      0.42     41188\n",
            "\n",
            "1 loop, best of 3: 486 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9jj_5i_9MDm",
        "colab_type": "text"
      },
      "source": [
        "## Model Comparison (GMM vs KModes) over Bank Marketing dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOKE7m6H2LzF",
        "colab_type": "text"
      },
      "source": [
        "#### **Classification report of GMM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhZJ9o962SgN",
        "colab_type": "code",
        "outputId": "7b11659d-3ae4-44f2-88da-b6092ec07e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print(classification_report(gmm_preds, label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.87      0.65     21850\n",
            "           1       0.40      0.10      0.15     19338\n",
            "\n",
            "    accuracy                           0.51     41188\n",
            "   macro avg       0.46      0.48      0.40     41188\n",
            "weighted avg       0.46      0.51      0.42     41188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX35CpTo2c61",
        "colab_type": "text"
      },
      "source": [
        "#### **Classification report of KModes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxZ_gn3_2Xjn",
        "colab_type": "code",
        "outputId": "ca718b47-0ff9-43a0-e2ad-d79e80e416b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print(classification_report(kmodes_clsuter_df['pred_cluster'], kmodes_clsuter_df['y']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.91      0.73     24497\n",
            "         1.0       0.51      0.14      0.22     16691\n",
            "\n",
            "    accuracy                           0.60     41188\n",
            "   macro avg       0.56      0.52      0.48     41188\n",
            "weighted avg       0.57      0.60      0.52     41188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9JI3nQQ3MvQ",
        "colab_type": "text"
      },
      "source": [
        "From the above results we can clearly see that the KModes algorithm is working better in clustering catrgorical data when the same amount of data is used to fit both the models.\n",
        "\n",
        "The final inference we get after performing all the operations are:\n",
        "- Kmodes work best with clustering categorical data\n",
        "- Kmodes is slow as compared to GMM\n",
        "- GMM gives bad result when clustering only with categorical inputs\n",
        "- Correlation and features selection plays a vital role in GMM and Kmodes both"
      ]
    }
  ]
}